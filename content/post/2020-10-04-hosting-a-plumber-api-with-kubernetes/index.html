---
title: Hosting a Plumber API with Kubernetes
author: ~
date: '2020-10-04'
slug: hosting-a-plumber-api-with-kubernetes
tags:
    - kubernetes
images: ["/img/kubernetes.png"]

---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<p>I’ve set myself an ambitious goal of building a Kubernetes cluster out of a couple of Raspberry Pis. This is pretty far out of my sphere of knowledge, so I have a lot to learn. I’ll be writing some posts to publish my notes and journal my experience publicly. In this post I’ll go through the basics of Kubernetes, and how I hosted a Plumber API in a Kubernetes cluster on Google Cloud Platform.</p>
<div id="the-basic-architecture" class="section level2">
<h2>The basic architecture</h2>
<div id="understand-the-container" class="section level3">
<h3>Understand the container</h3>
<p>The central concept of Kubernetes is the <em>container</em>. When I have an application that I want to deploy, I can bundle it up together with the software that it requires to run into a single blob called a container. That container can then be run on another machine that doesn’t have the application or its dependencies installed. The <em>Rocker</em> project, for example, provides containers with R or RStudio and all of their dependencies. I can run that container on another machine and access R or RStudio, without actually installing R or RStudio. The most common software for creating and running containers is called <em>Docker</em>.</p>
<p>Containers solve a lot of problems. I can bundle up an R script with the exact versions of the packages it uses to have a reproducible analysis. I can turn my R script into an API with <code>plumber</code> and put that in a container, so that running the container hosts the API. I can run multiple containers on the same machine, even if the applications are unrelated. If one of my applications requires version 3.6 of R, and another requires 4.0, then they can run side-by-side in containers.</p>
</div>
<div id="now-run-100-containers" class="section level3">
<h3>Now run 100 containers</h3>
<p>There’s complexity involved with running many containers, and it’s this complexity Kubernetes targets. I provide Kubernetes with a description of what I want the system to look like. I might ask for “3 copies of container A, and 2 copies of container B at all times” and Kubernetes will do its best to make that a reality.</p>
<p>Kubernetes is almost always run on a <em>cluster</em> of multiple machines. It can run copies of the same container (<em>replicas</em>) across multiple machines so that they can share a computational load, or so that if one falls over there’s another container ready to pick up the slack. In fact, Kubernetes doesn’t consider containers precious; they’re treated as ephemeral and replaceable. This also makes it easier to scale if there’s a spike in demand: just add more containers!<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></p>
<p>It’s not as simple as Kubernetes running containers, though. There are a few layers in between.</p>
<p>A <em>node</em> is a machine. It can be a Raspberry Pi, an old laptop, or a server with a six-figure price tag. Or it can be a virtual machine like an AWS EC2 instance.</p>
<p>At least one of these nodes is special. It contains the <em>control plane</em><a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a>, and it’s what coordinates the containers. The other nodes are called <em>worker</em> nodes, and it’s on these nodes that the containers are run.</p>
<p>There’s another layer in between node and container, and it’s the <em>pod</em>. Containers are grouped together in pods. Containers that rely on each other to perform a common goal are configured by the user to run in a pod together. A simple container that doesn’t rely on anything else can run in a pod by itself. In practice, the user configures pods, not containers.</p>
<p><img src="nodes.png" /></p>
</div>
</div>
<div id="sticking-a-plumber-api-in-a-container" class="section level2">
<h2>Sticking a Plumber API in a container</h2>
<div id="a-simple-plumber-api" class="section level3">
<h3>A simple Plumber API</h3>
<p>I started by making a container that runs a simple Plumber API. The <code>plumber</code> package lets me turn R functions into API endpoints. I created a <code>plumber.R</code> file that defines a few simple functions that I can use for testing:</p>
<ul>
<li><code>/parity</code> determines if a given integer is odd or even</li>
<li><code>/wait</code> waits 5 seconds, then returns the current time as a nicely formatted string</li>
<li><code>/fail</code> sets the <code>alive</code> global variable to <code>FALSE</code>.</li>
<li><code>/quit</code> runs <code>quit()</code>, exiting the R process that runs the API.</li>
<li><code>/health</code> returns “OK” if the <code>alive</code> global variable is <code>TRUE</code>, and throws an error otherwise.</li>
</ul>
<p>Note the special <code>#*</code> comments. These tell the <code>plumber</code> package how to map the function to an API endpoint. I’m also using futures for asynchronous calls, <a href="https://www.rplumber.io/news/">a feature introduced recently in Plumber 1.0.0</a>. R is fundamentally a single-threaded language, and so an API like this would normally only be able to serve one request at a time. The <code>future</code> package overcomes this.</p>
<pre class="r"><code># plumber.R
alive &lt;&lt;- TRUE

#* Determine if an integer is odd or even
#* @serializer text
#* @param int Integer to test for parity
#* @get /parity
function(int) {
  future({
    if (as.integer(int) %% 2 == 0) &quot;even&quot; else &quot;odd&quot;
  })
}

#* Wait 5 seconds and then return the current time
#* @serializer json
#* @get /wait
function() {
  future({
    Sys.sleep(5)
    list(time = Sys.time())
  })
}

#* Force the health check to fail
#* @post /fail
function() {
  alive &lt;&lt;- FALSE
  NULL
}

#* Try quitting
#* @post /quit
function() {
  quit()
}

#* Health check. Returns &quot;OK&quot;.
#* @serializer text
#* @get /health
function() {
  future({
    if (!alive) stop() else &quot;OK&quot;
  })
}</code></pre>
<p>I needed one other file: something that sets up dependencies, sources <code>plumber.R</code>, and runs the API. I called it <code>entrypoint.R</code>:</p>
<pre class="r"><code># entrypoint.R
library(plumber)
library(promises)
library(future)
future::plan(&quot;multiprocess&quot;)

pr(&quot;plumber.R&quot;) %&gt;% pr_run(host=&#39;0.0.0.0&#39;, port = 8000)</code></pre>
<p>At this point I had enough to run my API. On my laptop I could run <code>Rscript entrypoint.R</code> and the API would be hosted locally, accessible only through my machine. By packaging it into a Docker container I could start to think about moving my API onto a different machine.</p>
</div>
<div id="stick-it-in-a-container" class="section level3">
<h3>Stick it in a container</h3>
<p>To create a container<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>, I wrote out a “Dockerfile” that goes through the build process step-by-step. I can user Docker to <em>build</em> this Dockerfile into an <em>image</em>, which can then be run on another machine. <a href="https://raw.githubusercontent.com/mdneuzerling/plumber-on-k8s/4691f0ca34f8af9bb8258a61f8d113b169c6e8bb/Dockerfile">The full Dockerfile is available here</a>, but I’ll break it down below to make the process clearer.</p>
<p>Docker containers can build on one another. I start <code>FROM</code> the <code>rocker/r-ver:4.0.2</code>, <a href="https://www.rocker-project.org/">provided by the Rocker project</a>, which provides me with a Debian environment with R version 4.0.2 installed.</p>
<pre><code># Dockerfile
FROM rocker/r-ver:4.0.2</code></pre>
<p>Next I need to install some system dependencies required by <code>plumber</code>. <a href="https://packagemanager.rstudio.com/client/#/repos/1/packages/plumber">The RStudio Package Manager is a good resource for determining system dependencies</a>. Even though these system packages are already available on my laptop, they aren’t necessarily available in the <code>rocker</code> parent image, so I have to install them.</p>
<pre><code>RUN apt-get update -qq &amp;&amp; apt-get -y --no-install-recommends install \
    make \
    libsodium-dev \
    libicu-dev \
    libcurl4-openssl-dev \
    libssl-dev</code></pre>
<p>My API requires three R packages: <code>plumber</code>, <code>promises</code>, and <code>future</code>. I’ll install them using <a href="https://packagemanager.rstudio.com/client/#/repos/1/overview">the precompiled binaries provided by the RStudio Package Manager</a>. This makes the build process much faster, and also lets me pin the packages to the versions available as of a given date — in this case, 2020-10-01. I set the repository URL to an environment variable, which I then reference in R. An alternative approach here would be to add an <code>renv</code> lockfile to the image, and then <code>restore</code> it.</p>
<pre><code>ENV CRAN_REPO https://packagemanager.rstudio.com/all/__linux__/focal/338
RUN Rscript -e &#39;install.packages(c(&quot;plumber&quot;, &quot;promises&quot;, &quot;future&quot;), repos = c(&quot;CRAN&quot; = Sys.getenv(&quot;CRAN_REPO&quot;)))&#39;</code></pre>
<p>Every command in the Dockerfile is, by default, run with administrative privileges. That’s why I was able to run <code>apt-get install</code> without prefixing <code>sudo</code>. <a href="https://docs.docker.com/develop/develop-images/dockerfile_best-practices/#user">Docker suggests changing the user to one without root privileges if possible</a>, and my little Plumber API certainly doesn’t need root privileges. I’ll create a new <code>plumber</code> user, which I’ll later switch to with the <code>USER</code> command.</p>
<pre><code>RUN groupadd -r plumber &amp;&amp; useradd --no-log-init -r -g plumber plumber</code></pre>
<p>Now I’ll add the files I need to run my API. I’ll put them in the home directory of the <code>plumber</code> user I just created.</p>
<pre><code>ADD plumber.R /home/plumber/plumber.R
ADD entrypoint.R /home/plumber/entrypoint.R</code></pre>
<p>My API will be run through port 8000, but it will use port 8000 within the container. The <code>EXPOSE</code> command tells Docker that I intend to make this port available to the environment outside the container.</p>
<pre><code>EXPOSE 8000</code></pre>
<p>All of this setup makes the final steps fairly straightforward: change the working directory to the directory which contains my R files, switch to the <code>plumber</code> user, and run <code>entrypoint.R</code>.</p>
<pre><code>WORKDIR /home/plumber
USER plumber
CMD Rscript entrypoint.R</code></pre>
</div>
<div id="building-and-running-the-container" class="section level3">
<h3>Building and running the container</h3>
<p>To turn my Dockerfile into an image, I ran the following command in a shell:</p>
<pre><code>docker build -t mdneuzerling/plumber-on-k8s &lt;directory-with-files&gt;</code></pre>
<p>That last argument points to the directory containing <code>plumber.R</code> and <code>entrypoint.R</code>. If that’s my working directory, I can instead use <code>.</code> as this value. The <code>-t</code> is a tag, which is a convenient way of identifying the resulting image. It could be anything, but I set it to match <a href="https://github.com/mdneuzerling/plumber-on-k8s/">the Github repository in which these files are hosted</a>.</p>
<p>It can take a while to build an image, but Docker uses some smart caching along the way. If I change any line of the Dockerfile, only that line and what comes after will be rebuilt. For this reason, it’s generally good practice to put the stable time-consuming components of the Dockerfile (system dependencies, R package installations) towards the top of the file, and the components that are more likely to change towards the bottom.</p>
<p>To run my API, I use this command:</p>
<pre><code>docker run -p 8000:8000 mdneuzerling/plumber-on-k8s</code></pre>
<p>Containers have their own internal network of sorts. Earlier I exposed port 8000 in the container. Now I need to link port 8000 on my laptop to port 8000 within the container. This is what <code>-p 8000:8000</code> does.</p>
<p>Plumber APIs come with a pretty Swagger interface for testing the API. When I run the above command, I’m prompted to go to <code>http://127.0.0.1:8000/__docs__/</code> in my browser. And, sure enough, the API is there:</p>
<p><img src="swagger.png" /></p>
</div>
<div id="put-the-container-somewhere" class="section level3">
<h3>Put the container somewhere</h3>
<p>I needed to store my container somewhere accessible. A <em>Docker Registry</em> stores built Docker images, and is kind of like git for containers. <em>Docker Hub</em> is a registry provided by Docker itself, and offers a generous free tier.</p>
<p>I hadn’t used Docker Hub to host an image before, but I found the process very straightforward. <a href="https://hub.docker.com/repository/docker/mdneuzerling/plumber-on-k8s">I created a public repository for my image</a>, and then linked it to <a href="https://github.com/mdneuzerling/plumber-on-k8s/">a GitHub repository of the same name</a>. With very little effort I had a workflow in place such that pushing changes to my GitHub repository automatically rebuilt the image on Docker Hub!</p>
</div>
</div>
<div id="hosting-the-container-with-kubernetes" class="section level2">
<h2>Hosting the container with Kubernetes</h2>
<p>With my Raspberry Pi cluster components still in the mail, I had two options for trying out Kubernetes:</p>
<ol style="list-style-type: decimal">
<li>Use <code>minikube</code>, which sets up a cluster running on my laptop</li>
<li>Use a hosted service through a cloud provider</li>
</ol>
<p>I had some difficulty getting <code>minikube</code> to work, and I liked the idea of hosting my API in a publicly accessible location, so I went with a cloud provider. All three major providers offer hosted Kubernetes services, but I went with Google Kubernetes Engine on Google Cloud Platform There wasn’t an in-depth comparison here; I went with the service that looked like it had friendlier documentation.</p>
<div id="my-first-cluster" class="section level3">
<h3>My first cluster</h3>
<p>A hosted Kubernetes service means that some of the complexity of managing a cluster is abstracted away. In the case of Google Kubernetes Engine (GKE), it means that the control plane is handled for me. Every node I create will be a worker node. A warning here: every node is a Compute VM instance that costs money, but my little API can get by on very cheap instances: I selected g1-small (1 vCPU, 1.7 GB memory) VMs.</p>
<p>I won’t try to compete with <a href="https://cloud.google.com/kubernetes-engine/docs/quickstart">Google’s GKE documentation</a>. Instead, here’s a high-level overview of the steps I took to get my cluster up-and-running:</p>
<ol style="list-style-type: decimal">
<li>I created a new <em>project</em> for the cluster.</li>
<li>I went to the Google Kubernetes Engine page, which activated the API after a short wait.</li>
<li>I went to create a cluster, and Google suggested a “My First Cluster” configuration. This instantly created 3 nodes for me.</li>
<li>In a Cloud Shell, I configured my project and region. I also used <code>get-credentials</code> to connect to the cluster.</li>
</ol>
<p>From here on out I could use the <code>kubectl</code> command to configure and control my cluster from a Cloud Shell. I took note of the cluster’s external IP address as well — I would later use this to query my API.</p>
<p>The <code>kubectl</code> command contains everything I need to configure my cluster. I was using it so frequently that I was tempted to alias it to <code>k</code>. The big one is <code>get</code>, which lists all instances of the specified resource (I’ll explain deployments and services in a moment):</p>
<pre><code>kubectl get pods
kubectl get deployments
kubectl get services</code></pre>
<p>The other big one is <code>apply</code>. Kubernetes is most commonly configured by providing <code>kubectl</code> with YAML configuration files that tell the cluster how things should be. To “activate” one of these YAML files, I would write:</p>
<pre><code>kubectl apply -f &lt;file-location&gt;</code></pre>
<p>The <code>-f</code> tag indicates a file, but <code>&lt;file-location&gt;</code> could also be the URL of a YAML file. This was convenient for me, because I could point it towards the files as they were hosted on Github. My workflow was:</p>
<ol style="list-style-type: decimal">
<li>Edit the YAML file on my laptop</li>
<li>Commit and push to Github</li>
<li><code>kubectl apply</code></li>
</ol>
</div>
<div id="my-first-deployment" class="section level3">
<h3>My first deployment</h3>
<p>Pods, and the containers they hold, are created through a <em>deployment</em>. The YAML for a deployment specifies which pods to create and how many replicas to maintain, as well as various pieces of metadata. Here’s the deployment I used for my API:</p>
<pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  name: plumber-on-k8s-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: plumber-on-k8s
  template:
    metadata:
      labels:
        app: plumber-on-k8s
    spec:
      containers:
      - name: plumber-on-k8s
        image: mdneuzerling/plumber-on-k8s
        ports:
        - containerPort: 8000
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 3
          periodSeconds: 3
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 3
          periodSeconds: 3</code></pre>
<p>There’s a lot to unpack here, since not everything in this YAML is obvious:</p>
<ul>
<li>The <code>apiVersion</code> is used so that as the Kubernetes API updates and the expected format of these YAML files changes, the correct version can be used for the given configuration. I am unsure what “apps” means — it isn’t explained in any easily Google-able location.</li>
<li><code>replicas</code> refers to how many of the pods should be kept alive at all time. If a pod fails, Kubernetes will restart it to ensure that the number of available pods matches the specified number of <code>replicas</code>.</li>
<li>The <code>selector</code> tells Kubernetes which pods are part of this deployment. In this case, the pods are created in the same YAML file as the deployment, so it may seem somewhat redundant. But this isn’t always the case.</li>
<li>The <code>template</code> is how pods are specified. We use the <code>mdneuzerling/plumber-on-k8s</code> image, which is assumed to be from Docker Hub. However, I could just as easily refer to an image in a different or even private Docker registry.</li>
<li>The <code>containerPort</code> tells Kubernetes which ports are going to be used outside the container. I was confused at first by how often I have to specify port numbers! But given that there are so many <em>layers</em> of networks here (container, pod, cluster, external), I can understand why it is so important to be specific.</li>
</ul>
<p>The last two fields of interest are the probes. A <code>livenessProbe</code> provides a method for Kubernetes to determine if a container is alive and accepting traffic. In this case, I’m pointing it towards the <code>/health</code> endpoint I defined in <code>plumber.R</code>. I <em>believe</em> that all that is required from the endpoint is a status code of 200, and that the actual content of the response is irrelevant, but I couldn’t get a clear picture of this.</p>
<p>I also have a <code>readinessProbe</code>. While my container is simple and starts up pretty quickly, this isn’t true of all containers. Imagine a container that hosts a complicated machine learning model — it will take at least a few seconds to load the required model into memory. In this case, Kubernetes will wait until the <code>readinessProbe</code> succeeds before sending traffic to the pod.</p>
<p>In my case, the <code>livenessProbe</code> and <code>readinessProbe</code> both point towards the <code>/health</code> endpoint. This is because that endpoint will not be available until <code>plumber</code> has finished loading, so it serves the same purpose. This isn’t always the case for every API. The advantage of defining both is that the <code>livenessProbe</code> is ignored until the container reports as ready, subject to a timeout period.</p>
<p>A deployment of an API is, by itself, not particularly useful. I now had three pods, each hosting an API on port 8000. This isn’t possible on one network: in fact, each pod has a separate network. In order to get traffic flowing to this deployment, I needed to declare a <em>service</em>.</p>
</div>
<div id="my-first-service" class="section level3">
<h3>My first service</h3>
<p>Pods are ephemeral; they come and go. That’s why a <em>service</em> is required to describe how pods should be accessed. Here is the service I used:</p>
<pre><code>apiVersion: v1
kind: Service
metadata:
  name: plumber-on-k8s-service
spec:
  selector:
    app: plumber-on-k8s
  ports:
    - protocol: TCP
      port: 8000
      targetPort: 8000</code></pre>
<p><a href="https://kubernetes.io/docs/tutorials/kubernetes-basics/scale/scale-intro/">Services have an integrated load balance that will attempt to spread traffic evenly across the pods</a>. If a pod fails (as indicated by its <code>livenessProbe</code>) then traffic will be seamlessly routed to the available nodes.</p>
</div>
<div id="my-first-ingress" class="section level3">
<h3>My first ingress</h3>
<p>In order to actually query my API, there’s one last component I needed to put in place. An <em>ingress</em> directs traffic from outside the cluster to a service inside the cluster.</p>
<p>I found it difficult to find a configuration that would work. I suspect that the ingress API is under development at the moment (note the beta in the <code>apiVersion</code>), and may have changed significantly in version 1.19 of Kubernetes.</p>
<pre><code>apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: plumber-on-k8s-ingress
spec:
  rules:
  - http:
      paths:
      - path: /health
        backend:
          serviceName: plumber-on-k8s-service
          servicePort: 8000
      - path: /parity
        backend:
          serviceName: plumber-on-k8s-service
          servicePort: 8000
      - path: /wait
        backend:
          serviceName: plumber-on-k8s-service
          servicePort: 8000
      - path: /fail
        backend:
          serviceName: plumber-on-k8s-service
          servicePort: 8000
      - path: /quit
        backend:
          serviceName: plumber-on-k8s-service
          servicePort: 8000</code></pre>
<p>Here I’ve specified five paths which are directed to the <code>plumber-on-k8s-service</code> I previously defined. I could make the <code>spec</code> more compact with regular expressions, or by passing on all traffic regardless of path, but I chose to be explicit here.</p>
</div>
<div id="kubernetes-does-its-thing" class="section level3">
<h3>Kubernetes does its thing</h3>
<p>With a deployment, service and ingress defined, I now have everything in place to query my API. Using the external IP address (listed in the GCP console), I can send queries from my laptop:</p>
<pre><code>curl -X GET http://&lt;cluster-ip&gt;/parity?int=15
odd</code></pre>
<p>I can now also start to explore some of the features that come “for free” with using Kubernetes. Watch below what happens when I deliberately make a container fail its <code>livenessProbe</code> (I’ve censored some details in this screenshot):</p>
<p><img src="killing-a-container.png" /></p>
<p>It doesn’t matter that one of the pods is temporarily unable to accept traffic — the service routes all traffic to the remaining 2 available pods. Kubernetes notices that something is wrong: my <code>deployment.yaml</code> file asks for 3 replicas, but there are only 2 live pods. It then restarts the pod, so that reality reflects the configuration.</p>
<p>At one point I updated my Dockerfile, and I needed the deployment to fetch the new pods. I asked Kubernetes to do a “rolling restart”. It brought up new pods and took the old ones offline systematically, so that the service remained uninterrupted:</p>
<p><img src="rollout-restart.jpeg" /></p>
<hr />
<pre class="r"><code>devtools::session_info()</code></pre>
<pre><code>## ─ Session info ───────────────────────────────────────────────────────────────
##  setting  value                       
##  version  R version 4.1.0 (2021-05-18)
##  os       macOS Big Sur 11.3          
##  system   aarch64, darwin20           
##  ui       X11                         
##  language (EN)                        
##  collate  en_AU.UTF-8                 
##  ctype    en_AU.UTF-8                 
##  tz       Australia/Melbourne         
##  date     2021-05-30                  
## 
## ─ Packages ───────────────────────────────────────────────────────────────────
##  package     * version date       lib source        
##  blogdown      1.3     2021-04-14 [1] CRAN (R 4.1.0)
##  bookdown      0.22    2021-04-22 [1] CRAN (R 4.1.0)
##  bslib         0.2.5.1 2021-05-18 [1] CRAN (R 4.1.0)
##  cachem        1.0.4   2021-02-13 [1] CRAN (R 4.1.0)
##  callr         3.7.0   2021-04-20 [1] CRAN (R 4.1.0)
##  cli           2.5.0   2021-04-26 [1] CRAN (R 4.1.0)
##  crayon        1.4.1   2021-02-08 [1] CRAN (R 4.1.0)
##  desc          1.3.0   2021-03-05 [1] CRAN (R 4.1.0)
##  devtools      2.4.0   2021-04-07 [1] CRAN (R 4.1.0)
##  digest        0.6.27  2020-10-24 [1] CRAN (R 4.1.0)
##  ellipsis      0.3.2   2021-04-29 [1] CRAN (R 4.1.0)
##  evaluate      0.14    2019-05-28 [1] CRAN (R 4.1.0)
##  fastmap       1.1.0   2021-01-25 [1] CRAN (R 4.1.0)
##  fs            1.5.0   2020-07-31 [1] CRAN (R 4.1.0)
##  glue          1.4.2   2020-08-27 [1] CRAN (R 4.1.0)
##  htmltools     0.5.1.1 2021-01-22 [1] CRAN (R 4.1.0)
##  jquerylib     0.1.4   2021-04-26 [1] CRAN (R 4.1.0)
##  jsonlite      1.7.2   2020-12-09 [1] CRAN (R 4.1.0)
##  knitr         1.33    2021-04-24 [1] CRAN (R 4.1.0)
##  lifecycle     1.0.0   2021-02-15 [1] CRAN (R 4.1.0)
##  magrittr      2.0.1   2020-11-17 [1] CRAN (R 4.1.0)
##  memoise       2.0.0   2021-01-26 [1] CRAN (R 4.1.0)
##  pkgbuild      1.2.0   2020-12-15 [1] CRAN (R 4.1.0)
##  pkgload       1.2.1   2021-04-06 [1] CRAN (R 4.1.0)
##  prettyunits   1.1.1   2020-01-24 [1] CRAN (R 4.1.0)
##  processx      3.5.2   2021-04-30 [1] CRAN (R 4.1.0)
##  ps            1.6.0   2021-02-28 [1] CRAN (R 4.1.0)
##  purrr         0.3.4   2020-04-17 [1] CRAN (R 4.1.0)
##  R6            2.5.0   2020-10-28 [1] CRAN (R 4.1.0)
##  remotes       2.3.0   2021-04-01 [1] CRAN (R 4.1.0)
##  rlang         0.4.11  2021-04-30 [1] CRAN (R 4.1.0)
##  rmarkdown     2.8     2021-05-07 [1] CRAN (R 4.1.0)
##  rprojroot     2.0.2   2020-11-15 [1] CRAN (R 4.1.0)
##  sass          0.4.0   2021-05-12 [1] CRAN (R 4.1.0)
##  sessioninfo   1.1.1   2018-11-05 [1] CRAN (R 4.1.0)
##  stringi       1.6.1   2021-05-10 [1] CRAN (R 4.1.0)
##  stringr       1.4.0   2019-02-10 [1] CRAN (R 4.1.0)
##  testthat      3.0.2   2021-02-14 [1] CRAN (R 4.1.0)
##  usethis       2.0.1   2021-02-10 [1] CRAN (R 4.1.0)
##  withr         2.4.2   2021-04-18 [1] CRAN (R 4.1.0)
##  xfun          0.22    2021-03-11 [1] CRAN (R 4.1.0)
##  yaml          2.2.1   2020-02-01 [1] CRAN (R 4.1.0)
## 
## [1] /Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/library</code></pre>
<p><a href="https://github.com/kubernetes/community/tree/master/icons">The Kubernetes icons used in the chart that appears at the top of this page, and further below in the article, are used here under a CC-BY-4.0 licence.</a></p>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>I count at least three different ways to scale with Kubernetes. Adding more containers is just an example.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>The deprecated term for the node with the control plane is the <em>master</em> node. This is the only time I will refer to the node with this terminology.<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>There is a subtle difference here between a “container” and an “image”, but it’s a difference I’m going to disregard as it only confuses the situation without benefit.<a href="#fnref3" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
